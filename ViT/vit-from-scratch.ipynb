{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":33884,"sourceType":"datasetVersion","datasetId":1864}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nVision Transformer (ViT) Implementation from Scratch - Optimized for T4 GPU\n\nThis file contains a complete implementation of Vision Transformer (ViT) for image classification \nusing the Food-101 dataset. The implementation is built from scratch using PyTorch and optimized for T4 GPU.\n\nSections:\n1. Imports and Setup\n2. Helper Functions\n3. Dataset Handling\n4. ViT Components\n5. Complete ViT Model\n6. Training Functions\n7. Main Training Script\n\"\"\"\n\n# =============================================================================\n# 1. IMPORTS AND SETUP\n# =============================================================================\n\nimport torch\nimport torchvision\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom torch import nn\nfrom torchvision import transforms\nimport json\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport os\nimport random\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm.auto import tqdm\nfrom typing import List\n\n# Set device - prioritize CUDA for T4\ndevice = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Check GPU memory and capabilities for multi-GPU T4 optimization\nif device == \"cuda\":\n    num_gpus = torch.cuda.device_count()\n    print(f\"Number of GPUs available: {num_gpus}\")\n    \n    for i in range(num_gpus):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n        print(f\"GPU {i} Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n        print(f\"GPU {i} CUDA Capability: {torch.cuda.get_device_capability(i)}\")\n    \n    # Enable optimizations for T4\n    torch.backends.cudnn.benchmark = True  # Optimize for consistent input sizes\n    torch.backends.cuda.matmul.allow_tf32 = True  # Enable TF32 for faster training\n    torch.backends.cudnn.allow_tf32 = True\n    \n    # Multi-GPU setup\n    if num_gpus > 1:\n        print(f\"\\nðŸš€ Multi-GPU Training Enabled with {num_gpus} GPUs!\")\n        print(\"This will significantly speed up training and allow larger batch sizes.\")\n    else:\n        print(\"\\nâš¡ Single GPU Training\")\nelse:\n    num_gpus = 0\n\n# =============================================================================\n# 2. HELPER FUNCTIONS\n# =============================================================================\n\ndef plot_predictions(train_data, train_labels, test_data, test_labels, predictions=None):\n    \"\"\"\n    Plots linear training data and test data and compares predictions.\n    \"\"\"\n    plt.figure(figsize=(10, 7))\n    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n    \n    if predictions is not None:\n        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n    \n    plt.legend(prop={\"size\": 14})\n\n\ndef accuracy_fn(y_true, y_pred):\n    \"\"\"Calculates accuracy between truth labels and predictions.\n\n    Args:\n        y_true (torch.Tensor): Truth labels for predictions.\n        y_pred (torch.Tensor): Predictions to be compared to predictions.\n\n    Returns:\n        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n    \"\"\"\n    correct = torch.eq(y_true, y_pred).sum().item()\n    acc = (correct / len(y_pred)) * 100\n    return acc\n\n\ndef print_train_time(start, end, device=None):\n    \"\"\"Prints difference between start and end time.\n\n    Args:\n        start (float): Start time of computation (preferred in timeit format). \n        end (float): End time of computation.\n        device ([type], optional): Device that compute is running on. Defaults to None.\n\n    Returns:\n        float: time between start and end in seconds (higher is longer).\n    \"\"\"\n    total_time = end - start\n    print(f\"\\nTrain time on {device}: {total_time:.3f} seconds\")\n    return total_time\n\n\ndef plot_loss_curves(results):\n    \"\"\"Plots training curves of a results dictionary.\n\n    Args:\n        results (dict): dictionary containing list of values, e.g.\n            {\"train_loss\": [...],\n             \"train_acc\": [...],\n             \"test_loss\": [...],\n             \"test_acc\": [...]}\n    \"\"\"\n    loss = results[\"train_loss\"]\n    test_loss = results[\"test_loss\"]\n    \n    accuracy = results[\"train_acc\"]\n    test_accuracy = results[\"test_acc\"]\n    \n    epochs = range(len(results[\"train_loss\"]))\n    \n    plt.figure(figsize=(15, 7))\n    \n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, label=\"train_loss\")\n    plt.plot(epochs, test_loss, label=\"test_loss\")\n    plt.title(\"Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n    \n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, accuracy, label=\"train_accuracy\")\n    plt.plot(epochs, test_accuracy, label=\"test_accuracy\")\n    plt.title(\"Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n\n\ndef pred_and_plot_image(\n    model: torch.nn.Module,\n    image_path: str,\n    class_names: List[str] = None,\n    transform=None,\n    device: torch.device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n):\n    \"\"\"Makes a prediction on a target image with a trained model and plots the image.\n\n    Args:\n        model (torch.nn.Module): trained PyTorch image classification model.\n        image_path (str): filepath to target image.\n        class_names (List[str], optional): different class names for target image. Defaults to None.\n        transform (_type_, optional): transform of target image. Defaults to None.\n        device (torch.device, optional): target device to compute on. Defaults to \"cuda\" if torch.cuda.is_available() else \"cpu\".\n    \n    Returns:\n        Matplotlib plot of target image and model prediction as title.\n\n    Example usage:\n        pred_and_plot_image(model=model,\n                            image=\"some_image.jpeg\",\n                            class_names=[\"class_1\", \"class_2\", \"class_3\"],\n                            transform=torchvision.transforms.ToTensor(),\n                            device=device)\n    \"\"\"\n    \n    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n    target_image = target_image / 255.0\n    \n    if transform:\n        target_image = transform(target_image)\n    \n    model.to(device)\n    model.eval()\n    \n    with torch.inference_mode():\n        target_image = target_image.unsqueeze(dim=0)\n        target_image_pred = model(target_image.to(device))\n    \n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n    \n    plt.imshow(target_image.squeeze().permute(1, 2, 0)) \n    \n    if class_names:\n        title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    else:\n        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    \n    plt.title(title)\n    plt.axis(False)\n\n\ndef set_seeds(seed: int = 42):\n    \"\"\"Sets random sets for torch operations.\n\n    Args:\n        seed (int, optional): Random seed to set. Defaults to 42.\n    \"\"\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # For multi-GPU\n    np.random.seed(seed)\n    random.seed(seed)\n\n\ndef get_gpu_memory_usage():\n    \"\"\"Returns current GPU memory usage in GB.\"\"\"\n    if torch.cuda.is_available():\n        return torch.cuda.memory_allocated() / 1024**3\n    return 0\n\n# =============================================================================\n# 3. DATASET HANDLING\n# =============================================================================\n\nclass Food101Dataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        label = self.labels[idx]\n        \n        # Load image\n        image = Image.open(image_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        return image, label\n\n\ndef load_split(data_path, split_file):\n    with open(data_path / \"meta/meta\" / split_file, 'r') as f:\n        split_data = json.load(f)\n    return split_data\n\n\ndef prepare_data(data_path):\n    # Load splits\n    train_split = load_split(data_path, \"train.json\")\n    test_split = load_split(data_path, \"test.json\")\n\n    # Prepare data\n    train_images = []\n    train_labels = []\n    test_images = []\n    test_labels = []\n    \n    # Process train split\n    for class_name, image_paths in train_split.items():\n        for relative_img_path in image_paths:  # e.g., \"poutine/1005364\"\n            # The relative_img_path from the JSON (e.g., \"poutine/1005364\")\n            # already contains the class folder.\n            # We just need to append \".jpg\" and join with \"images\" directory.\n            train_images.append(data_path / \"images\" / f\"{relative_img_path}.jpg\")\n            train_labels.append(list(train_split.keys()).index(class_name))\n\n    # Process test split\n    for class_name, image_paths in test_split.items():\n        for relative_img_path in image_paths:  # e.g., \"poutine/1005364\"\n            # Same logic as for the training split.\n            test_images.append(data_path / \"images\" / f\"{relative_img_path}.jpg\")\n            test_labels.append(list(test_split.keys()).index(class_name))\n\n    return train_images, train_labels, test_images, test_labels\n\n# =============================================================================\n# 4. VIT COMPONENTS\n# =============================================================================\n\nclass PatchEmbeddings(nn.Module):\n    \"\"\"\n    Converts input images into patch embeddings.\n    \"\"\"\n    def __init__(self,\n                 in_channels: int = 3,\n                 embeddings_dimensions: int = 512,  # Reduced for T4\n                 patch_size: int = 16):\n        super().__init__()\n        \n        self.patch_size = patch_size\n        self.patched_embeddings = nn.Conv2d(in_channels=in_channels,\n                                          out_channels=embeddings_dimensions,\n                                          stride=patch_size,\n                                          padding=0,\n                                          kernel_size=patch_size)\n\n        self.flatten_embeddings = nn.Flatten(start_dim=2, end_dim=3)\n\n    def forward(self, x):\n        image_resolution = x.shape[-1]\n        assert image_resolution % self.patch_size == 0, f\"Input image size must be divisible by patch size, image shape: {image_resolution}, patch size: {self.patch_size}\"\n\n        x_patched = self.patched_embeddings(x)\n        x_flatten = self.flatten_embeddings(x_patched)\n        return x_flatten.permute(0, 2, 1)\n\n\nclass MultiHeadSelfAttentionBlock(nn.Module):\n    \"\"\"\n    Multi-Head Self-Attention mechanism for Vision Transformer.\n    \"\"\"\n    def __init__(self,\n                 num_heads: int = 8,  # Reduced for T4\n                 embeddings_dimension: int = 512,  # Reduced for T4\n                 attn_dropout: float = 0.1):\n        super().__init__()\n\n        self.layer_norm = nn.LayerNorm(embeddings_dimension)\n        self.multihead_attn_layer = nn.MultiheadAttention(\n            embed_dim=embeddings_dimension,\n            num_heads=num_heads,\n            dropout=attn_dropout,\n            batch_first=True\n        )\n\n    def forward(self, x):\n        x = self.layer_norm(x)\n        attn_output, _ = self.multihead_attn_layer(query=x, key=x, value=x, need_weights=False)\n        return attn_output\n\n\nclass MLPBlock(nn.Module):\n    \"\"\"\n    Multi-Layer Perceptron block with GELU activation.\n    \"\"\"\n    def __init__(self,\n                 embeddings_dimension: int = 512,  # Reduced for T4\n                 mlp_size: int = 2048,  # Reduced for T4\n                 dropout: float = 0.1):\n        super().__init__()\n\n        self.layer_norm = nn.LayerNorm(normalized_shape=embeddings_dimension)\n\n        self.mlp = nn.Sequential(\n            nn.Linear(in_features=embeddings_dimension,\n                     out_features=mlp_size),\n            nn.GELU(),\n            nn.Dropout(p=dropout),\n            nn.Linear(in_features=mlp_size,\n                     out_features=embeddings_dimension),\n            nn.Dropout(p=dropout)\n        )\n\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = self.mlp(x)\n        return x\n\n\nclass TransformerEncoderBlock(nn.Module):\n    \"\"\"\n    Complete Transformer Encoder block with self-attention and MLP.\n    \"\"\"\n    def __init__(self,\n                 num_heads: int = 8,  # Reduced for T4\n                 embeddings_dimension: int = 512,  # Reduced for T4\n                 dropout: float = 0.1,\n                 mlp_size: int = 2048,  # Reduced for T4\n                 attn_dropout: float = 0.1):\n        super().__init__()\n\n        self.msa_layer = MultiHeadSelfAttentionBlock(\n            num_heads=num_heads,\n            embeddings_dimension=embeddings_dimension,\n            attn_dropout=attn_dropout\n        )\n\n        self.mlp_block = MLPBlock(\n            dropout=dropout,\n            embeddings_dimension=embeddings_dimension,\n            mlp_size=mlp_size\n        )\n\n    def forward(self, x):\n        x = self.msa_layer(x) + x  # Residual connection\n        x = self.mlp_block(x) + x  # Residual connection\n        return x\n\n# =============================================================================\n# 5. COMPLETE VIT MODEL\n# =============================================================================\n\nclass ViT(nn.Module):\n    \"\"\"\n    Complete Vision Transformer (ViT) model implementation optimized for T4 GPU.\n    \"\"\"\n    def __init__(self,\n                 num_heads: int = 8,  # Reduced for T4\n                 embeddings_dimension: int = 512,  # Reduced for T4\n                 dropout: float = 0.1,\n                 mlp_size: int = 2048,  # Reduced for T4\n                 attn_dropout: float = 0.1,\n                 num_of_encoder_layers: int = 8,  # Reduced for T4\n                 patch_size: int = 16,\n                 image_width: int = 224,\n                 img_height: int = 224,\n                 no_channels: int = 3,\n                 classes: int = 101,\n                 positional_embedding_dropout: float = 0.1):\n        \n        assert (img_height * image_width) % (patch_size * patch_size) == 0, \\\n            f\"Image dimensions ({img_height}x{image_width}) must be divisible by patch_size squared ({patch_size*patch_size})\"\n\n        super().__init__()\n        self.number_of_patches = (image_width * img_height) // (patch_size * patch_size)\n\n        self.patch_embeddings = PatchEmbeddings(\n            in_channels=no_channels,\n            embeddings_dimensions=embeddings_dimension,\n            patch_size=patch_size\n        )\n\n        self.positional_embeddings = nn.Parameter(\n            torch.randn(1, self.number_of_patches + 1, embeddings_dimension),\n            requires_grad=True\n        )\n\n        self.cls_token = nn.Parameter(\n            torch.randn(1, 1, embeddings_dimension),\n            requires_grad=True\n        )\n\n        self.encoder_block = nn.Sequential(*[\n            TransformerEncoderBlock(\n                num_heads=num_heads,\n                embeddings_dimension=embeddings_dimension,\n                dropout=dropout,\n                mlp_size=mlp_size,\n                attn_dropout=attn_dropout\n            ) for _ in range(num_of_encoder_layers)\n        ])\n\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(embeddings_dimension),\n            nn.Linear(in_features=embeddings_dimension, out_features=classes)\n        )\n\n        self.dropout_after_positional_embeddings = nn.Dropout(p=positional_embedding_dropout)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        x = self.patch_embeddings(x)\n        prepend_token = self.cls_token.expand(batch_size, -1, -1)\n\n        x = torch.cat((prepend_token, x), dim=1)\n        x = self.positional_embeddings + x\n        x = self.dropout_after_positional_embeddings(x)\n        x = self.encoder_block(x)\n        x = self.classifier(x[:, 0])\n\n        return x\n\n# =============================================================================\n# 6. TRAINING FUNCTIONS\n# =============================================================================\n\ndef get_data_transforms():\n    \"\"\"\n    Returns data transformations for training - optimized for T4.\n    \"\"\"\n    return transforms.Compose([\n        transforms.Resize(size=(224, 224)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomRotation(degrees=10),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n\ndef train_step(model, dataloader, loss_fn, optimizer, device, scaler=None, num_gpus=1):\n    \"\"\"\n    Performs one training step (epoch) with mixed precision and multi-GPU support for T4.\n    \"\"\"\n    model.train()\n    train_loss, train_acc = 0, 0\n    \n    train_pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n    \n    for batch, (X, y) in enumerate(train_pbar):\n        # Send data to target device\n        X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n\n        # Mixed precision training for T4\n        if scaler is not None and device == \"cuda\":\n            with torch.amp.autocast(device_type='cuda'):\n                # Forward pass\n                y_pred = model(X)\n                loss = loss_fn(y_pred, y)\n            \n            # Backward pass with gradient scaling\n            optimizer.zero_grad()\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            # Regular training\n            y_pred = model(X)\n            loss = loss_fn(y_pred, y)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        train_loss += loss.item()\n\n        # Calculate and accumulate accuracy metric across all batches\n        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        current_acc = accuracy_fn(y_true=y, y_pred=y_pred_class)\n        train_acc += current_acc\n\n        # Update progress bar with memory usage\n        if device == \"cuda\":\n            memory_usage = sum(torch.cuda.memory_allocated(i) for i in range(num_gpus)) / 1024**3\n            train_pbar.set_postfix(\n                loss=loss.item(), \n                acc=f\"{current_acc:.2f}%\",\n                mem=f\"{memory_usage:.1f}GB\"\n            )\n        else:\n            train_pbar.set_postfix(loss=loss.item(), acc=f\"{current_acc:.2f}%\")\n\n    # Calculate average metrics for this epoch\n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    \n    return train_loss, train_acc\n\n\ndef test_step(model, dataloader, loss_fn, device, num_gpus=1):\n    \"\"\"\n    Performs one testing step with memory optimization and multi-GPU support.\n    \"\"\"\n    model.eval()\n    test_loss, test_acc = 0, 0\n    \n    with torch.inference_mode():\n        test_pbar = tqdm(dataloader, desc=\"Testing\", leave=False)\n        \n        for batch, (X, y) in enumerate(test_pbar):\n            # Send data to target device\n            X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n\n            # Mixed precision inference\n            if device == \"cuda\":\n                with torch.amp.autocast(device_type='cuda'):\n                    test_pred_logits = model(X)\n                    loss = loss_fn(test_pred_logits, y)\n            else:\n                test_pred_logits = model(X)\n                loss = loss_fn(test_pred_logits, y)\n\n            test_loss += loss.item()\n\n            # Calculate and accumulate accuracy\n            test_pred_labels = test_pred_logits.argmax(dim=1)\n            current_acc = accuracy_fn(y_true=y, y_pred=test_pred_labels)\n            test_acc += current_acc\n\n            # Update progress bar\n            if device == \"cuda\":\n                memory_usage = sum(torch.cuda.memory_allocated(i) for i in range(num_gpus)) / 1024**3\n                test_pbar.set_postfix(\n                    loss=loss.item(), \n                    acc=f\"{current_acc:.2f}%\",\n                    mem=f\"{memory_usage:.1f}GB\"\n                )\n            else:\n                test_pbar.set_postfix(loss=loss.item(), acc=f\"{current_acc:.2f}%\")\n\n    # Calculate final test metrics\n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc / len(dataloader)\n    \n    return test_loss, test_acc\n\n# =============================================================================\n# 7. MAIN TRAINING SCRIPT - OPTIMIZED FOR MULTI-GPU T4 SETUP\n# =============================================================================\n\ndef main():\n    \"\"\"\n    Main training function optimized for multi-GPU T4 setup.\n    \"\"\"\n    print(f\"Using device: {device}\")\n    \n    # Get number of GPUs\n    if device == \"cuda\":\n        num_gpus = torch.cuda.device_count()\n    else:\n        num_gpus = 0\n    \n    # Set up data paths - Fixed for Kaggle environment\n    import os\n    \n    # Try different possible paths for Kaggle environment\n    possible_paths = [\n        Path(\"/kaggle/input/food41\"),  # Kaggle input path\n        Path(\"../input/food41\"),      # Alternative Kaggle path\n        Path(\"./food41\"),              # Local path\n        Path(\"food41\"),                # Current directory\n        Path(\"./food-101\"),            # Original naming\n        Path(\"food-101\")               # Original naming alternative\n    ]\n    \n    data_path = None\n    for path in possible_paths:\n        if path.exists():\n            data_path = path\n            print(f\"Found dataset at: {data_path}\")\n            break\n    \n    if data_path is None:\n        # List available directories to help debug\n        print(\"Available directories in current location:\")\n        for item in Path(\".\").iterdir():\n            if item.is_dir():\n                print(f\"  - {item}\")\n        print(\"\\nAvailable directories in /kaggle/input (if exists):\")\n        kaggle_input = Path(\"/kaggle/input\")\n        if kaggle_input.exists():\n            for item in kaggle_input.iterdir():\n                if item.is_dir():\n                    print(f\"  - {item}\")\n        \n        raise FileNotFoundError(\"Could not find food dataset. Please check the dataset path.\")\n\n    # Define training parameters optimized for multi-GPU T4 setup\n    NUM_EPOCHS = 25  # Can maintain or even increase with better GPU utilization\n    \n    # Optimize batch size for multi-GPU setup\n    if num_gpus >= 2:\n        BATCH_SIZE = 32  # Increased for 2 T4 GPUs (16 per GPU)\n        LEARNING_RATE = 2e-4  # Slightly increased for larger effective batch size\n        print(f\"ðŸš€ Multi-GPU Setup: Using batch size {BATCH_SIZE} ({BATCH_SIZE//num_gpus} per GPU)\")\n    else:\n        BATCH_SIZE = 16  # Single GPU batch size\n        LEARNING_RATE = 1e-4\n        print(f\"âš¡ Single GPU Setup: Using batch size {BATCH_SIZE}\")\n    \n    # Define checkpoint directory - Fixed for Kaggle\n    run_name = f\"vit_food101_multi_t4_optimized\" if num_gpus > 1 else f\"vit_food101_t4_optimized\"\n    \n    # For Kaggle, save checkpoints in working directory\n    if \"/kaggle\" in str(Path.cwd()):\n        checkpoint_base_dir = Path(\"/kaggle/working/checkpoints\")\n    else:\n        checkpoint_base_dir = Path(\"./checkpoints\")\n    \n    checkpoint_dir = checkpoint_base_dir / run_name\n    \n    # Get data transforms\n    data_transform = get_data_transforms()\n    \n    # Prepare data\n    train_images, train_labels, test_images, test_labels = prepare_data(data_path)\n    \n    print(f\"Total training images available: {len(train_images)}\")\n    print(f\"Using the full training dataset ({len(train_images)} images) for {NUM_EPOCHS} epochs\")\n    print(f\"Batch size optimized for {num_gpus} GPU(s): {BATCH_SIZE}\")\n    \n    # Create datasets\n    full_train_dataset = Food101Dataset(train_images, train_labels, transform=data_transform)\n    test_dataset = Food101Dataset(test_images, test_labels, transform=data_transform)\n    \n    # Create dataloaders with optimized settings for multi-GPU T4\n    # Increase num_workers for better data loading with multiple GPUs\n    num_workers = min(8, os.cpu_count() // max(1, num_gpus)) if num_gpus > 1 else 4\n    \n    test_dataloader = DataLoader(\n        dataset=test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n        persistent_workers=True  # Keep workers alive between epochs\n    )\n    \n    train_dataloader = DataLoader(\n        dataset=full_train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n        persistent_workers=True  # Keep workers alive between epochs\n    )\n    \n    # Create model optimized for T4\n    vit = ViT(\n        classes=len(set(train_labels)),\n        embeddings_dimension=512,  # Maintained for T4\n        num_heads=8,  # Maintained for T4\n        num_of_encoder_layers=8,  # Maintained for T4\n        mlp_size=2048  # Maintained for T4\n    )\n    \n    # Multi-GPU setup with DataParallel\n    if num_gpus > 1:\n        print(f\"\\nðŸ”¥ Setting up DataParallel for {num_gpus} GPUs...\")\n        vit = nn.DataParallel(vit)\n        print(\"âœ… DataParallel setup complete!\")\n    \n    vit = vit.to(device)\n    \n    # Count parameters\n    if num_gpus > 1:\n        # For DataParallel, access the original model via .module\n        total_params = sum(p.numel() for p in vit.module.parameters())\n        trainable_params = sum(p.numel() for p in vit.module.parameters() if p.requires_grad)\n    else:\n        total_params = sum(p.numel() for p in vit.parameters())\n        trainable_params = sum(p.numel() for p in vit.parameters() if p.requires_grad)\n    \n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,}\")\n    if num_gpus > 1:\n        print(f\"Parameters per GPU: ~{trainable_params//num_gpus:,}\")\n    \n    # Setup optimizer with multi-GPU optimized settings\n    optimizer = torch.optim.AdamW(\n        params=vit.parameters(),\n        lr=LEARNING_RATE,\n        weight_decay=0.05,  # Slightly increased for regularization\n        betas=(0.9, 0.999)\n    )\n    \n    # Learning rate scheduler\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, \n        T_max=NUM_EPOCHS,\n        eta_min=1e-6\n    )\n    \n    loss_fn = nn.CrossEntropyLoss()\n    \n    # Mixed precision scaler for T4\n    scaler = torch.amp.GradScaler(device='cuda') if device == \"cuda\" else None\n    \n    # Create a writer instance - Fixed for Kaggle\n    if \"/kaggle\" in str(Path.cwd()):\n        log_dir = f\"/kaggle/working/runs/{run_name}\"\n    else:\n        log_dir = f\"runs/{run_name}\"\n    \n    writer = SummaryWriter(log_dir=log_dir)\n    \n    # Training loop\n    results = {\"train_loss\": [], \"train_acc\": []}\n    \n    print(\"\\n\" + \"=\"*60)\n    if num_gpus > 1:\n        print(f\"STARTING MULTI-GPU TRAINING ON {num_gpus} T4 GPUs (Mixed Precision)\")\n    else:\n        print(\"STARTING TRAINING ON T4 GPU (Mixed Precision)\")\n    print(\"=\"*60)\n    \n    for epoch in range(NUM_EPOCHS):\n        print(f\"\\n=== Epoch {epoch + 1}/{NUM_EPOCHS} ===\")\n        \n        # Clear cache before each epoch for all GPUs\n        if device == \"cuda\":\n            for i in range(num_gpus):\n                with torch.cuda.device(i):\n                    torch.cuda.empty_cache()\n        \n        # Train for one epoch\n        train_loss, train_acc = train_step(vit, train_dataloader, loss_fn, optimizer, device, scaler, num_gpus)\n        \n        # Update learning rate\n        scheduler.step()\n        current_lr = optimizer.param_groups[0]['lr']\n        \n        # Store results\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        \n        # Log to TensorBoard\n        writer.add_scalar(tag=\"Loss/train\", scalar_value=train_loss, global_step=epoch)\n        writer.add_scalar(tag=\"Accuracy/train\", scalar_value=train_acc, global_step=epoch)\n        writer.add_scalar(tag=\"Learning Rate\", scalar_value=current_lr, global_step=epoch)\n        \n        # Log GPU memory usage for all GPUs\n        if device == \"cuda\":\n            total_memory = 0\n            for i in range(num_gpus):\n                memory_used = torch.cuda.max_memory_allocated(i) / 1024**3\n                total_memory += memory_used\n                writer.add_scalar(tag=f\"GPU_{i}_Memory_(GB)\", scalar_value=memory_used, global_step=epoch)\n                torch.cuda.reset_peak_memory_stats(i)\n            \n            writer.add_scalar(tag=\"Total_GPU_Memory_(GB)\", scalar_value=total_memory, global_step=epoch)\n        \n        print(f\"Epoch {epoch+1} | Train loss: {train_loss:.4f} | Train acc: {train_acc:.2f}% | LR: {current_lr:.6f}\")\n        if num_gpus > 1:\n            print(f\"         | Multi-GPU Memory: {total_memory:.1f}GB total ({total_memory/num_gpus:.1f}GB avg)\")\n        \n        # Save checkpoint every 5 epochs and at the end\n        if (epoch + 1) % 5 == 0 or epoch == NUM_EPOCHS - 1:\n            if checkpoint_dir:\n                checkpoint_dir.mkdir(parents=True, exist_ok=True)\n                \n                # Handle DataParallel model state dict\n                model_state_dict = vit.module.state_dict() if num_gpus > 1 else vit.state_dict()\n                \n                checkpoint = {\n                    'epoch': epoch + 1,\n                    'model_state_dict': model_state_dict,\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scheduler_state_dict': scheduler.state_dict(),\n                    'scaler_state_dict': scaler.state_dict() if scaler else None,\n                    'train_loss': train_loss,\n                    'train_acc': train_acc,\n                    'num_gpus': num_gpus\n                }\n                torch.save(checkpoint, checkpoint_dir / f\"epoch_{epoch+1}_checkpoint.pth\")\n                torch.save(model_state_dict, checkpoint_dir / \"latest_model.pth\")\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"TRAINING COMPLETED - STARTING FINAL TESTING\")\n    print(\"=\"*60)\n    \n    # Final testing phase\n    test_loss, test_acc = test_step(vit, test_dataloader, loss_fn, device, num_gpus)\n    \n    # Add final test results to results dict\n    results[\"test_loss\"] = [test_loss]\n    results[\"test_acc\"] = [test_acc]\n    \n    # Log final test results\n    writer.add_scalar(tag=\"Loss/final_test\", scalar_value=test_loss, global_step=NUM_EPOCHS)\n    writer.add_scalar(tag=\"Accuracy/final_test\", scalar_value=test_acc, global_step=NUM_EPOCHS)\n    \n    print(f\"\\nFINAL RESULTS:\")\n    print(f\"Final Test Loss: {test_loss:.4f}\")\n    print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n    \n    # Save final model\n    if checkpoint_dir:\n        # Handle DataParallel model state dict\n        model_state_dict = vit.module.state_dict() if num_gpus > 1 else vit.state_dict()\n        \n        final_checkpoint = {\n            'model_state_dict': model_state_dict,\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'test_loss': test_loss,\n            'test_acc': test_acc,\n            'num_gpus': num_gpus,\n            'model_config': {\n                'embeddings_dimension': 512,\n                'num_heads': 8,\n                'num_of_encoder_layers': 8,\n                'mlp_size': 2048,\n                'classes': len(set(train_labels))\n            }\n        }\n        torch.save(final_checkpoint, checkpoint_dir / \"final_checkpoint.pth\")\n        torch.save(model_state_dict, checkpoint_dir / \"final_model.pth\")\n        print(f\"Final model saved to {checkpoint_dir / 'final_model.pth'}\")\n    \n    # Close the writer\n    writer.close()\n    \n    # Plot training curves\n    plt.figure(figsize=(12, 5))\n    \n    # Plot training loss\n    plt.subplot(1, 2, 1)\n    plt.plot(range(1, NUM_EPOCHS + 1), results[\"train_loss\"], label=\"Train Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    title_suffix = f\"(Multi-GPU: {num_gpus} T4s)\" if num_gpus > 1 else \"(Single T4)\"\n    plt.title(f\"Training Loss {title_suffix}\")\n    plt.legend()\n    plt.grid(True)\n    \n    # Plot training accuracy (and final test accuracy)\n    plt.subplot(1, 2, 2)\n    plt.plot(range(1, NUM_EPOCHS + 1), results[\"train_acc\"], label=\"Train Accuracy\")\n    if results.get(\"test_acc\"):\n        plt.axhline(y=results[\"test_acc\"][0], color='r', linestyle='--', \n                    label=f'Final Test Acc: {results[\"test_acc\"][0]:.2f}%')\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy (%)\")\n    plt.title(f\"Training vs Test Accuracy {title_suffix}\")\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    if checkpoint_dir:\n        plt.savefig(checkpoint_dir / \"training_curves.png\", dpi=300, bbox_inches='tight')\n    else:\n        plt.savefig(\"training_curves.png\", dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"\\nMulti-GPU T4 Optimized Training Summary:\")\n    print(f\"Number of GPUs used: {num_gpus}\")\n    print(f\"Total epochs: {NUM_EPOCHS}\")\n    print(f\"Training dataset size: {len(full_train_dataset)}\")\n    print(f\"Batch size: {BATCH_SIZE} (total) / {BATCH_SIZE//max(1,num_gpus)} (per GPU)\")\n    print(f\"Model parameters: {trainable_params:,}\")\n    print(f\"Mixed precision: {'Enabled' if scaler else 'Disabled'}\")\n    print(f\"DataParallel: {'Enabled' if num_gpus > 1 else 'Disabled'}\")\n    print(f\"Total training samples processed: {NUM_EPOCHS * len(full_train_dataset)}\")\n    if results.get(\"test_acc\"):\n        print(f\"Final test accuracy: {results['test_acc'][0]:.2f}%\")\n\n\n# =============================================================================\n# 8. KAGGLE/NOTEBOOK EXECUTION FUNCTIONS\n# =============================================================================\n\ndef run_training():\n    \"\"\"\n    Function to run training in Kaggle/Notebook environment.\n    Call this function instead of main() in notebooks.\n    \"\"\"\n    # Set random seeds for reproducibility\n    set_seeds(42)\n    \n    # Run the main training function\n    main()\n\n\ndef example_inference():\n    \"\"\"\n    Example function showing how to use the trained multi-GPU model for inference.\n    Optimized for T4 GPU inference.\n    \"\"\"\n    # Load the trained model - handles both single and multi-GPU trained models\n    # model = ViT(\n    #     classes=101,\n    #     embeddings_dimension=512,\n    #     num_heads=8,\n    #     num_of_encoder_layers=8,\n    #     mlp_size=2048\n    # )\n    # \n    # # Load checkpoint\n    # checkpoint = torch.load(\"path/to/your/checkpoint.pth\", map_location=device)\n    # \n    # # Handle both single-GPU and multi-GPU trained models\n    # if 'num_gpus' in checkpoint and checkpoint['num_gpus'] > 1:\n    #     # Model was trained with DataParallel\n    #     model.load_state_dict(checkpoint['model_state_dict'])\n    # else:\n    #     # Model was trained on single GPU\n    #     model.load_state_dict(checkpoint['model_state_dict'])\n    # \n    # # For inference, we don't need DataParallel (use single GPU)\n    # model.to(device)\n    # model.eval()\n    \n    # Load class names (you'll need to create this list based on your dataset)\n    # class_names = [\"class_1\", \"class_2\", ...]  # Replace with actual class names\n    \n    # Make a prediction on a sample image with mixed precision\n    # with torch.amp.autocast(device_type='cuda'):\n    #     pred_and_plot_image(model=model,\n    #                         image_path=\"path/to/your/image.jpg\",\n    #                         class_names=class_names,\n    #                         transform=get_data_transforms(),\n    #                         device=device)\n    pass\n\n\ndef load_multi_gpu_model(checkpoint_path, num_classes=101):\n    \"\"\"\n    Utility function to properly load a model trained with multi-GPU setup.\n    \n    Args:\n        checkpoint_path (str): Path to the checkpoint file\n        num_classes (int): Number of classes in the model\n    \n    Returns:\n        model: Loaded model ready for inference\n    \"\"\"\n    # Create model\n    model = ViT(\n        classes=num_classes,\n        embeddings_dimension=512,\n        num_heads=8,\n        num_of_encoder_layers=8,\n        mlp_size=2048\n    )\n    \n    # Load checkpoint\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    \n    # Load model state dict\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    # Move to device and set to eval mode\n    model.to(device)\n    model.eval()\n    \n    print(f\"âœ… Model loaded from {checkpoint_path}\")\n    if 'num_gpus' in checkpoint:\n        print(f\"ðŸ“Š Model was trained using {checkpoint['num_gpus']} GPU(s)\")\n    if 'test_acc' in checkpoint:\n        print(f\"ðŸŽ¯ Final test accuracy: {checkpoint['test_acc']:.2f}%\")\n    \n    return model\n\n\n# =============================================================================\n# 9. SCRIPT EXECUTION\n# =============================================================================\n\nif __name__ == \"__main__\":\n    # This will only run when executing as a script, not in notebook\n    run_training()\n\n# For Kaggle/Notebook users, uncomment the line below:\n# run_training() ","metadata":{"_uuid":"db40bad1-18be-4cae-bf76-2965a097ac1d","_cell_guid":"ad782e5c-a80b-4961-858f-771cb5206b4c","trusted":true,"execution":{"iopub.status.busy":"2025-06-04T20:01:22.900948Z","iopub.execute_input":"2025-06-04T20:01:22.901237Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"Using device: cuda\nNumber of GPUs available: 2\nGPU 0: Tesla T4\nGPU 0 Memory: 14.7 GB\nGPU 0 CUDA Capability: (7, 5)\nGPU 1: Tesla T4\nGPU 1 Memory: 14.7 GB\nGPU 1 CUDA Capability: (7, 5)\n\nðŸš€ Multi-GPU Training Enabled with 2 GPUs!\nThis will significantly speed up training and allow larger batch sizes.\nUsing device: cuda\nFound dataset at: /kaggle/input/food41\nðŸš€ Multi-GPU Setup: Using batch size 32 (16 per GPU)\nTotal training images available: 75750\nUsing the full training dataset (75750 images) for 25 epochs\nBatch size optimized for 2 GPU(s): 32\n\nðŸ”¥ Setting up DataParallel for 2 GPUs...\nâœ… DataParallel setup complete!\nTotal parameters: 25,767,013\nTrainable parameters: 25,767,013\nParameters per GPU: ~12,883,506\n\n============================================================\nSTARTING MULTI-GPU TRAINING ON 2 T4 GPUs (Mixed Precision)\n============================================================\n\n=== Epoch 1/25 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/2368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1 | Train loss: 4.2028 | Train acc: 6.67% | LR: 0.000199\n         | Multi-GPU Memory: 1.8GB total (0.9GB avg)\n\n=== Epoch 2/25 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/2368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2 | Train loss: 3.8185 | Train acc: 12.52% | LR: 0.000197\n         | Multi-GPU Memory: 1.8GB total (0.9GB avg)\n\n=== Epoch 3/25 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/2368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3 | Train loss: 3.5897 | Train acc: 16.50% | LR: 0.000193\n         | Multi-GPU Memory: 1.8GB total (0.9GB avg)\n\n=== Epoch 4/25 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/2368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4 | Train loss: 3.3833 | Train acc: 20.22% | LR: 0.000188\n         | Multi-GPU Memory: 1.8GB total (0.9GB avg)\n\n=== Epoch 5/25 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/2368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5 | Train loss: 3.1889 | Train acc: 23.67% | LR: 0.000181\n         | Multi-GPU Memory: 1.8GB total (0.9GB avg)\n\n=== Epoch 6/25 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/2368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6 | Train loss: 3.0037 | Train acc: 27.25% | LR: 0.000173\n         | Multi-GPU Memory: 1.8GB total (0.9GB avg)\n\n=== Epoch 7/25 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/2368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7 | Train loss: 2.8267 | Train acc: 30.64% | LR: 0.000164\n         | Multi-GPU Memory: 1.8GB total (0.9GB avg)\n\n=== Epoch 8/25 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/2368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 8 | Train loss: 2.6623 | Train acc: 34.19% | LR: 0.000154\n         | Multi-GPU Memory: 1.8GB total (0.9GB avg)\n\n=== Epoch 9/25 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/2368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 9 | Train loss: 2.5064 | Train acc: 37.43% | LR: 0.000143\n         | Multi-GPU Memory: 1.8GB total (0.9GB avg)\n\n=== Epoch 10/25 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/2368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10 | Train loss: 2.3496 | Train acc: 40.53% | LR: 0.000131\n         | Multi-GPU Memory: 1.8GB total (0.9GB avg)\n\n=== Epoch 11/25 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/2368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 11 | Train loss: 2.1911 | Train acc: 44.16% | LR: 0.000119\n         | Multi-GPU Memory: 1.8GB total (0.9GB avg)\n\n=== Epoch 12/25 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/2368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 12 | Train loss: 2.0336 | Train acc: 47.35% | LR: 0.000107\n         | Multi-GPU Memory: 1.8GB total (0.9GB avg)\n\n=== Epoch 13/25 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/2368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 13 | Train loss: 1.8707 | Train acc: 51.17% | LR: 0.000094\n         | Multi-GPU Memory: 1.8GB total (0.9GB avg)\n\n=== Epoch 14/25 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/2368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da306f85b71c4a03b4a2e88c5c735180"}},"metadata":{}}],"execution_count":null}]}